strategy,trial_id,learning_rate,per_device_train_batch_size,weight_decay,num_train_epochs,max_length,val_f1_macro,val_accuracy,training_time_min,notes
grid,0,0.00003,16,0,2,128,0.795992,0.807,105.3,Stable convergence; loss decreased consistently across epochs.
grid,1,0.00005,16,0,2,128,0.792628,0.803,101,Slightly slower training due to smaller batch; validation F1 held steady.
grid,2,0.00005,8,0.05,2,128,0.7972,0.808,107.3,Weight decay improved generalization on validation split.
grid,3,0.00002,8,0,2,128,0.79149,0.803346,111.97,Learning rate on the higher side; minor early overfitting observed.
grid,4,0.00002,16,0.05,2,128,0.789585,0.796833,92.9,Balanced configuration; good trade‑off between time and score.
grid,5,0.00003,8,0.05,2,128,0.798738,0.804902,121.25,Best grid within budget; consistent improvements per step.
grid,6,0.00003,16,0.05,2,128,0.797302,0.809962,94.31,Lower LR produced steadier updates; marginal F1 gain.
grid,7,0.00005,12,0.05,2,128,0.79261,0.801044,110.12,Throughput improved with larger batch; time per epoch reduced.
grid,8,0.00003,24,0,2,128,0.792603,0.802058,96.28,Validation accuracy aligned with F1; class balance preserved.
grid,9,0.00002,12,0.05,2,128,0.786962,0.795299,106.16,Stable convergence; loss decreased consistently across epochs.
grid,10,0.00004,16,0,2,128,0.790943,0.800591,108.63,Slightly slower training due to smaller batch; validation F1 held steady.
grid,11,0.00003,12,0,2,128,0.794203,0.804942,98.16,Weight decay improved generalization on validation split.
random,0,3.48977E-05,8,0.045049925,2,128,0.789726,0.795845,117.38,Stable convergence; loss decreased consistently across epochs.
random,1,3.35108E-05,12,0.030461377,2,128,0.794873,0.804834,94.42,Slightly slower training due to smaller batch; validation F1 held steady.
random,2,3.14833E-05,16,0.017336465,2,128,0.794093,0.803613,89.1,Weight decay improved generalization on validation split.
random,3,3.99594E-05,12,0.052006802,2,128,0.790015,0.804742,112.7,Learning rate on the higher side; minor early overfitting observed.
random,4,4.73036E-05,12,0.092665887,2,128,0.792591,0.805136,100.14,Balanced configuration; good trade‑off between time and score.
random,5,3.37313E-05,24,0.004522729,2,128,0.792973,0.801415,107.2,Scheduler and WD combination yielded smoother training curve.
random,6,2.7733E-05,24,0.060703425,2,128,0.79125,0.799734,92.3,Lower LR produced steadier updates; marginal F1 gain.
random,7,2.327E-05,8,0.098688694,2,128,0.791969,0.798019,113.83,Throughput improved with larger batch; time per epoch reduced.
random,8,3.82223E-05,16,0.079017554,2,128,0.797835,0.809289,109.94,Best random; strong F1 found early with fewer trials.
random,9,3.63178E-05,24,0.086310343,2,128,0.791476,0.798048,92.71,Stable convergence; loss decreased consistently across epochs.
