owner,pipeline_stage,model,variant,num_train_epochs,batch_size,learning_rate,weight_decay,max_seq_length,warmup_steps,scheduler,grad_accum,train_samples,val_accuracy,val_f1_macro,val_precision_macro,val_recall_macro,val_loss,test_accuracy,test_f1_macro,test_precision_macro,test_recall_macro,test_loss,runtime_minutes,notes
Candelaria,data-prep,tfidf-logreg,baseline,1,128,0,0,0,0,constant,1,4955,0.8084,0.8075,0.8089,0.8062,0.4921,0.8052,0.8041,0.8063,0.8017,0.5893,0.48,Baseline TF-IDF logistic regression with default preprocessing.
Candelaria,feature-search,tfidf-logreg,char_wb_3_5,1,128,0,0,0,0,constant,1,4955,0.8262,0.8341,0.8334,0.8352,0.4685,0.8218,0.8306,0.8271,0.8324,0.5719,0.51,Character n-gram windowing (3-5) improved recall on neutral class.
Candelaria,feature-search,tfidf-logreg,word_1_2,1,128,0,0,0,0,constant,1,4955,0.9481,0.9576,0.9684,0.9541,0.3483,0.9352,0.9621,0.9655,0.9714,0.4622,0.49,Word bi-gram expansion; cross-validation flagged strong precision gains.
Candelaria,augmentation,tfidf-logreg,smote-rebalanced,1,128,0,0,0,0,constant,1,5210,0.8343,0.8428,0.8467,0.8394,0.4576,0.8294,0.8371,0.8405,0.8348,0.5634,0.62,SMOTE interpolation on minority sentiment prior to TF-IDF training.
Candelaria,transformer,xlm-roberta-base,auto-grid,2.8,8,0,0.044,256,200,linear,1,4955,0.8719,0.8947,0.8879,0.8995,0.4312,0.8783,0.9026,0.9018,0.9007,0.5284,1.68,Optuna grid-search best trial with early stopping patience=2.
Candelaria,transformer,xlm-roberta-base,auto-random,1.7,16,0,0.0316,256,50,cosine,4,4955,0.8549,0.8346,0.8382,0.8263,0.4461,0.8588,0.8315,0.8336,0.8292,0.5411,1.73,Random search sample with cosine decay; retained mixed precision.
Candelaria,transformer,xlm-roberta-base,finetune-256,3.3,8,0,0.0418,256,80,linear,2,4955,0.9341,0.9186,0.9211,0.9197,0.3627,0.9304,0.9092,0.9058,0.9079,0.4698,2.42,Full fine-tuning with layer-wise LR decay (0.95).
Candelaria,ablation,xlm-roberta-base,reduced-maxlen,2.6,8,0,0.0228,128,100,cosine,1,4955,0.8427,0.8682,0.8695,0.8654,0.4598,0.8513,0.8691,0.8684,0.8719,0.5538,2.18,Sequence length capped at 128 tokens; minimal drop observed.
Candelaria,ablation,xlm-roberta-base,class-weighted,2.3,16,0,0.0084,128,200,linear,1,4955,0.8816,0.8937,0.8942,0.8699,0.4182,0.8735,0.9036,0.9064,0.8981,0.5261,1.46,Inverse-frequency class weighting stabilized minority predictions.
Candelaria,distillation,distilroberta-base,teacher-xlmr,4,32,0,0.012,256,150,polynomial,2,4955,0.8894,0.8762,0.8817,0.8741,0.4075,0.8831,0.8714,0.8742,0.8692,0.5183,1.28,Distillation from best XLM-R checkpoint with temperature=2.5.
Candelaria,domain-adaptation,xlm-roberta-base,mlm-news-10k,2.5,16,0,0.019,320,120,linear,2,4955,0.9037,0.9112,0.9153,0.9074,0.3895,0.8979,0.9061,0.9097,0.9028,0.5057,1.97,Continued MLM on 10k in-domain reviews prior to supervised fine-tune.
Candelaria,ensembling,stacked,xlmr+tfidf,1,0,0,0,0,0,n/a,1,4955,0.9365,0.9413,0.9448,0.9385,0.3314,0.9317,0.9368,0.9393,0.9349,0.4496,0.92,Stacked ensemble blending TF-IDF and XLM-R logits with logistic meta-learner.
