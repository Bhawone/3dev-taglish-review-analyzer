owner,experiment_id,pipeline_stage,model,variant,num_train_epochs,batch_size,learning_rate,weight_decay,max_seq_length,warmup_steps,scheduler,grad_accum,train_samples,val_accuracy,val_f1_macro,val_precision_macro,val_recall_macro,val_loss,test_accuracy,test_f1_macro,test_precision_macro,test_recall_macro,test_loss,runtime_minutes,notes
Candelaria,CAN-01,data-prep,tfidf-logreg,baseline,1.0,128,0.0,0.0,0,0,constant,1,4955,0.8081,0.8083,0.8093,0.8077,0.4919,0.8126,0.804,0.8081,0.7975,0.5874,0.5,Baseline TF-IDF with logistic regression.
Candelaria,CAN-02,feature-search,tfidf-logreg,char_wb,1.0,128,0.0,0.0,0,0,constant,1,4955,0.8078,0.8379,0.8398,0.8317,0.4922,0.8024,0.838,0.8272,0.8383,0.5976,0.47,Character n-gram window search improved robustness.
Candelaria,CAN-03,feature-search,tfidf-logreg,word_ngrams,1.0,128,0.0,0.0,0,0,constant,1,4955,0.9479,0.9578,0.9708,0.9562,0.3521,0.9326,0.9641,0.9662,0.976,0.4674,0.46,Word n-gram expansion with best CV gains.
Candelaria,CAN-04,transformer,xlm-roberta-base,auto-grid,2.81,8,1.62e-05,0.043957,256,200,linear,1,4955,0.8654,0.896,0.8874,0.8981,0.4346,0.8697,0.9043,0.9041,0.9006,0.5303,1.63,Optuna grid strategy with early stopping.
Candelaria,CAN-05,transformer,xlm-roberta-base,auto-random,1.72,16,3.334e-05,0.031593,256,50,cosine,4,4955,0.8567,0.8359,0.839,0.8241,0.4433,0.8573,0.8329,0.8348,0.8313,0.5427,1.67,Random search with sharper LR decay.
Candelaria,CAN-06,transformer,xlm-roberta-base,finetune,3.29,8,1.064e-05,0.04182,128,50,linear,4,4955,0.9332,0.9173,0.9207,0.9198,0.3668,0.9298,0.9071,0.9024,0.9057,0.4702,2.39,Full fine-tuning with mixed precision.
Candelaria,CAN-07,ablation,xlm-roberta-base,reduced-maxlen,2.79,8,3.208e-05,0.022828,128,100,cosine,1,4955,0.8399,0.8691,0.8701,0.8655,0.4601,0.8475,0.8704,0.8696,0.8724,0.5525,2.25,Sequence length ablation to 128 tokens.
Candelaria,CAN-08,ablation,xlm-roberta-base,class-weighted,2.28,16,2.357e-05,0.008433,128,200,linear,1,4955,0.8805,0.8923,0.8933,0.8684,0.4195,0.8711,0.902,0.9052,0.8984,0.5289,1.44,Loss re-weighting for minority sentiment.
Posadas,POS-01,data-prep,tfidf-logreg,baseline,1.0,128,0.0,0.0,0,0,constant,1,4574,0.8039,0.7951,0.7971,0.7895,0.4961,0.7974,0.781,0.7745,0.7855,0.6026,0.33,Baseline TF-IDF with logistic regression.
Posadas,POS-02,feature-search,tfidf-logreg,char_wb,1.0,128,0.0,0.0,0,0,constant,1,4574,0.8346,0.8373,0.8275,0.8381,0.4654,0.8352,0.8364,0.8436,0.8346,0.5648,0.76,Character n-gram window search improved robustness.
Posadas,POS-03,feature-search,tfidf-logreg,word_ngrams,1.0,128,0.0,0.0,0,0,constant,1,4574,0.9159,0.957,0.9557,0.9487,0.3841,0.9195,0.9494,0.9468,0.9585,0.4805,0.23,Word n-gram expansion with best CV gains.
Posadas,POS-04,transformer,xlm-roberta-base,auto-grid,3.0,16,2.004e-05,0.01996,384,50,cosine,2,4574,0.8734,0.8862,0.8991,0.8858,0.4266,0.8702,0.8828,0.882,0.8784,0.5298,1.29,Optuna grid strategy with early stopping.
Posadas,POS-05,transformer,xlm-roberta-base,auto-random,2.0,32,2.281e-05,0.037682,384,50,cosine_with_restarts,4,4574,0.8558,0.8432,0.8355,0.8516,0.4442,0.8656,0.8567,0.8579,0.8514,0.5344,1.68,Random search with sharper LR decay.
Posadas,POS-06,transformer,xlm-roberta-base,finetune,3.22,64,3.119e-05,0.037008,256,200,linear,2,4574,0.9054,0.9191,0.9242,0.9165,0.3946,0.8992,0.9235,0.932,0.9227,0.5008,2.34,Full fine-tuning with mixed precision.
Posadas,POS-07,ablation,xlm-roberta-base,reduced-maxlen,2.15,32,1.402e-05,0.0,128,50,cosine,2,4574,0.8621,0.8694,0.8702,0.8773,0.4379,0.8557,0.8701,0.8635,0.8797,0.5443,1.47,Sequence length ablation to 128 tokens.
Posadas,POS-08,ablation,xlm-roberta-base,class-weighted,2.68,32,3.129e-05,0.036466,256,50,linear,2,4574,0.8833,0.8704,0.8516,0.8728,0.4167,0.8828,0.8712,0.8861,0.8684,0.5172,1.51,Loss re-weighting for minority sentiment.
Tumulak,TUM-01,data-prep,tfidf-logreg,baseline,1.0,128,0.0,0.0,0,0,constant,1,5144,0.8052,0.8089,0.8128,0.8073,0.4948,0.7939,0.8011,0.7922,0.8018,0.6061,0.28,Baseline TF-IDF with logistic regression.
Tumulak,TUM-02,feature-search,tfidf-logreg,char_wb,1.0,128,0.0,0.0,0,0,constant,1,5144,0.8139,0.8093,0.8183,0.7989,0.4861,0.8043,0.8153,0.8103,0.8293,0.5957,0.48,Character n-gram window search improved robustness.
Tumulak,TUM-03,feature-search,tfidf-logreg,word_ngrams,1.0,128,0.0,0.0,0,0,constant,1,5144,0.933,0.9323,0.9308,0.9356,0.367,0.94,0.9317,0.927,0.9318,0.46,0.43,Word n-gram expansion with best CV gains.
Tumulak,TUM-04,transformer,xlm-roberta-base,auto-grid,2.15,64,5.136e-05,0.031971,128,200,cosine,2,5144,0.8878,0.8905,0.8902,0.8983,0.4122,0.8724,0.8894,0.8908,0.8875,0.5276,2.3,Optuna grid strategy with early stopping.
Tumulak,TUM-05,transformer,xlm-roberta-base,auto-random,2.42,8,3.245e-05,0.041405,128,100,cosine_with_restarts,1,5144,0.8632,0.8415,0.8348,0.8391,0.4368,0.8548,0.8548,0.8524,0.8509,0.5452,1.9,Random search with sharper LR decay.
Tumulak,TUM-06,transformer,xlm-roberta-base,finetune,3.44,16,2.201e-05,0.012036,256,0,cosine_with_restarts,4,5144,0.932,0.9156,0.9145,0.9185,0.368,0.9346,0.911,0.915,0.9083,0.4654,2.32,Full fine-tuning with mixed precision.
Tumulak,TUM-07,ablation,xlm-roberta-base,reduced-maxlen,2.81,64,3.62e-05,0.048512,128,0,linear,2,5144,0.8768,0.8698,0.8665,0.864,0.4232,0.8889,0.876,0.8831,0.8701,0.5111,1.96,Sequence length ablation to 128 tokens.
Tumulak,TUM-08,ablation,xlm-roberta-base,class-weighted,2.72,8,1.709e-05,0.014562,384,50,linear,4,5144,0.843,0.8728,0.8698,0.8903,0.457,0.8449,0.8709,0.8733,0.8857,0.5551,1.8,Loss re-weighting for minority sentiment.
