owner,pipeline_stage,model,variant,num_train_epochs,batch_size,learning_rate,weight_decay,max_seq_length,warmup_steps,scheduler,grad_accum,train_samples,val_accuracy,val_f1_macro,val_precision_macro,val_recall_macro,val_loss,test_accuracy,test_f1_macro,test_precision_macro,test_recall_macro,test_loss,runtime_minutes,notes
Posadas,data-prep,tfidf-svm,baseline,1,128,0,0,0,0,constant,1,4955,0.8012,0.7994,0.8038,0.7951,0.5036,0.7968,0.7945,0.7981,0.7917,0.5978,0.44,Baseline linear SVM with TF-IDF and default tokenization.
Posadas,feature-search,tfidf-svm,char_2_4,1,128,0,0,0,0,constant,1,4955,0.8174,0.8248,0.8263,0.8229,0.4781,0.8127,0.8201,0.8215,0.8184,0.5795,0.46,Character n-grams (2-4) added; slight gain on macro F1.
Posadas,feature-search,tfidf-svm,sublinear_tf,1,128,0,0,0,0,constant,1,4955,0.8291,0.8354,0.8386,0.8319,0.4638,0.8257,0.8321,0.8354,0.8288,0.5632,0.47,Sublinear TF scaling with IDF smoothing.
Posadas,augmentation,tfidf-svm,backtranslation,1,128,0,0,0,0,constant,1,5480,0.8382,0.8469,0.8475,0.8441,0.4527,0.8344,0.8426,0.8448,0.8395,0.5518,0.69,Englishâ†”Tagalog backtranslation augmentation for minority classes.
Posadas,transformer,xlm-roberta-base,frozen-encoder,2,32,0,0.01,256,100,linear,1,4955,0.8647,0.8725,0.8716,0.8732,0.4379,0.8615,0.8683,0.8692,0.8667,0.5336,1.34,Encoder frozen for first epoch; classifier head trained warm-up.
Posadas,transformer,xlm-roberta-base,layer-wise-lr,3,16,0,0.018,256,200,linear,2,4955,0.9078,0.9129,0.9154,0.9098,0.3825,0.9013,0.9071,0.9095,0.9043,0.4942,2.08,Layer-wise LR decay (0.9) with gradient checkpointing enabled.
Posadas,transformer,xlm-roberta-large,full-finetune,2.4,8,0,0.027,320,300,cosine,4,4955,0.9186,0.9251,0.9273,0.9219,0.3614,0.9128,0.9206,0.9231,0.9184,0.4823,2.87,Large model fine-tune with sequence length 320 and fp16.
Posadas,regularization,xlm-roberta-base,mixout-0.15,3.1,16,0,0.05,256,180,cosine,2,4955,0.8997,0.9062,0.9106,0.9019,0.3948,0.8951,0.9028,0.9057,0.8996,0.5027,2.21,Applied mixout regularization=0.15 with R-Drop penalty 0.5.
Posadas,distillation,miniLM,teacher-xlmr,4,32,0,0.015,256,120,linear,1,4955,0.8723,0.8611,0.8678,0.8583,0.4159,0.8675,0.8572,0.8616,0.8549,0.5214,1.36,Distilled MiniLM from POS-07 teacher with KL factor 0.5.
Posadas,domain-adaptation,xlm-roberta-base,mlm-social-5k,2.2,16,0,0.021,320,140,linear,2,4955,0.9089,0.9157,0.9185,0.9126,0.3796,0.9038,0.9104,0.9132,0.9078,0.4907,1.91,Secondary MLM on 5k social media posts improves robustness to slang.
Posadas,ensembling,soft-vote,xlmr+svm,1,0,0,0,0,0,n/a,1,4955,0.9264,0.9317,0.9341,0.9292,0.3446,0.9193,0.9245,0.9267,0.9226,0.4712,0.88,Soft-voting ensemble between POS-06 and POS-03 predictions.
Posadas,calibration,xlm-roberta-base,temperature-1.7,0.5,64,0,0,0,0,n/a,1,800,0.9126,0.9194,0.9226,0.9165,0.3728,0.9078,0.9141,0.9163,0.9117,0.4865,0.37,Post-hoc temperature scaling (T=1.7) on validation logits only.
