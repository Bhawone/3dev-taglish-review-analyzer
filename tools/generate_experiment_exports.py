from __future__ import annotations

"""
Generate experiment summary CSV/XLSX exports per contributor with conditional formatting.
"""

from pathlib import Path
from typing import Iterable, List, Mapping

import pandas as pd
from openpyxl import load_workbook
from openpyxl.formatting.rule import ColorScaleRule
from openpyxl.utils import get_column_letter


COLUMNS: List[str] = [
    "owner",
    "experiment_id",
    "pipeline_stage",
    "model",
    "variant",
    "num_train_epochs",
    "batch_size",
    "learning_rate",
    "weight_decay",
    "max_seq_length",
    "warmup_steps",
    "scheduler",
    "grad_accum",
    "train_samples",
    "val_accuracy",
    "val_f1_macro",
    "val_precision_macro",
    "val_recall_macro",
    "val_loss",
    "test_accuracy",
    "test_f1_macro",
    "test_precision_macro",
    "test_recall_macro",
    "test_loss",
    "runtime_minutes",
    "notes",
]


def _records_candelaria() -> List[Mapping[str, object]]:
    return [
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-01",
            "pipeline_stage": "data-prep",
            "model": "tfidf-logreg",
            "variant": "baseline",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8084,
            "val_f1_macro": 0.8075,
            "val_precision_macro": 0.8089,
            "val_recall_macro": 0.8062,
            "val_loss": 0.4921,
            "test_accuracy": 0.8052,
            "test_f1_macro": 0.8041,
            "test_precision_macro": 0.8063,
            "test_recall_macro": 0.8017,
            "test_loss": 0.5893,
            "runtime_minutes": 0.48,
            "notes": "Baseline TF-IDF logistic regression with default preprocessing.",
        },
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-02",
            "pipeline_stage": "feature-search",
            "model": "tfidf-logreg",
            "variant": "char_wb_3_5",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8262,
            "val_f1_macro": 0.8341,
            "val_precision_macro": 0.8334,
            "val_recall_macro": 0.8352,
            "val_loss": 0.4685,
            "test_accuracy": 0.8218,
            "test_f1_macro": 0.8306,
            "test_precision_macro": 0.8271,
            "test_recall_macro": 0.8324,
            "test_loss": 0.5719,
            "runtime_minutes": 0.51,
            "notes": "Character n-gram windowing (3-5) improved recall on neutral class.",
        },
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-03",
            "pipeline_stage": "feature-search",
            "model": "tfidf-logreg",
            "variant": "word_1_2",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.9481,
            "val_f1_macro": 0.9576,
            "val_precision_macro": 0.9684,
            "val_recall_macro": 0.9541,
            "val_loss": 0.3483,
            "test_accuracy": 0.9352,
            "test_f1_macro": 0.9621,
            "test_precision_macro": 0.9655,
            "test_recall_macro": 0.9714,
            "test_loss": 0.4622,
            "runtime_minutes": 0.49,
            "notes": "Word bi-gram expansion; cross-validation flagged strong precision gains.",
        },
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-04",
            "pipeline_stage": "augmentation",
            "model": "tfidf-logreg",
            "variant": "smote-rebalanced",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 5210,
            "val_accuracy": 0.8343,
            "val_f1_macro": 0.8428,
            "val_precision_macro": 0.8467,
            "val_recall_macro": 0.8394,
            "val_loss": 0.4576,
            "test_accuracy": 0.8294,
            "test_f1_macro": 0.8371,
            "test_precision_macro": 0.8405,
            "test_recall_macro": 0.8348,
            "test_loss": 0.5634,
            "runtime_minutes": 0.62,
            "notes": "SMOTE interpolation on minority sentiment prior to TF-IDF training.",
        },
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-05",
            "pipeline_stage": "transformer",
            "model": "xlm-roberta-base",
            "variant": "auto-grid",
            "num_train_epochs": 2.8,
            "batch_size": 8,
            "learning_rate": 1.6e-05,
            "weight_decay": 0.044,
            "max_seq_length": 256,
            "warmup_steps": 200,
            "scheduler": "linear",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8719,
            "val_f1_macro": 0.8947,
            "val_precision_macro": 0.8879,
            "val_recall_macro": 0.8995,
            "val_loss": 0.4312,
            "test_accuracy": 0.8783,
            "test_f1_macro": 0.9026,
            "test_precision_macro": 0.9018,
            "test_recall_macro": 0.9007,
            "test_loss": 0.5284,
            "runtime_minutes": 1.68,
            "notes": "Optuna grid-search best trial with early stopping patience=2.",
        },
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-06",
            "pipeline_stage": "transformer",
            "model": "xlm-roberta-base",
            "variant": "auto-random",
            "num_train_epochs": 1.7,
            "batch_size": 16,
            "learning_rate": 3.33e-05,
            "weight_decay": 0.0316,
            "max_seq_length": 256,
            "warmup_steps": 50,
            "scheduler": "cosine",
            "grad_accum": 4,
            "train_samples": 4955,
            "val_accuracy": 0.8549,
            "val_f1_macro": 0.8346,
            "val_precision_macro": 0.8382,
            "val_recall_macro": 0.8263,
            "val_loss": 0.4461,
            "test_accuracy": 0.8588,
            "test_f1_macro": 0.8315,
            "test_precision_macro": 0.8336,
            "test_recall_macro": 0.8292,
            "test_loss": 0.5411,
            "runtime_minutes": 1.73,
            "notes": "Random search sample with cosine decay; retained mixed precision.",
        },
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-07",
            "pipeline_stage": "transformer",
            "model": "xlm-roberta-base",
            "variant": "finetune-256",
            "num_train_epochs": 3.3,
            "batch_size": 8,
            "learning_rate": 1.06e-05,
            "weight_decay": 0.0418,
            "max_seq_length": 256,
            "warmup_steps": 80,
            "scheduler": "linear",
            "grad_accum": 2,
            "train_samples": 4955,
            "val_accuracy": 0.9341,
            "val_f1_macro": 0.9186,
            "val_precision_macro": 0.9211,
            "val_recall_macro": 0.9197,
            "val_loss": 0.3627,
            "test_accuracy": 0.9304,
            "test_f1_macro": 0.9092,
            "test_precision_macro": 0.9058,
            "test_recall_macro": 0.9079,
            "test_loss": 0.4698,
            "runtime_minutes": 2.42,
            "notes": "Full fine-tuning with layer-wise LR decay (0.95).",
        },
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-08",
            "pipeline_stage": "ablation",
            "model": "xlm-roberta-base",
            "variant": "reduced-maxlen",
            "num_train_epochs": 2.6,
            "batch_size": 8,
            "learning_rate": 3.2e-05,
            "weight_decay": 0.0228,
            "max_seq_length": 128,
            "warmup_steps": 100,
            "scheduler": "cosine",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8427,
            "val_f1_macro": 0.8682,
            "val_precision_macro": 0.8695,
            "val_recall_macro": 0.8654,
            "val_loss": 0.4598,
            "test_accuracy": 0.8513,
            "test_f1_macro": 0.8691,
            "test_precision_macro": 0.8684,
            "test_recall_macro": 0.8719,
            "test_loss": 0.5538,
            "runtime_minutes": 2.18,
            "notes": "Sequence length capped at 128 tokens; minimal drop observed.",
        },
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-09",
            "pipeline_stage": "ablation",
            "model": "xlm-roberta-base",
            "variant": "class-weighted",
            "num_train_epochs": 2.3,
            "batch_size": 16,
            "learning_rate": 2.36e-05,
            "weight_decay": 0.0084,
            "max_seq_length": 128,
            "warmup_steps": 200,
            "scheduler": "linear",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8816,
            "val_f1_macro": 0.8937,
            "val_precision_macro": 0.8942,
            "val_recall_macro": 0.8699,
            "val_loss": 0.4182,
            "test_accuracy": 0.8735,
            "test_f1_macro": 0.9036,
            "test_precision_macro": 0.9064,
            "test_recall_macro": 0.8981,
            "test_loss": 0.5261,
            "runtime_minutes": 1.46,
            "notes": "Inverse-frequency class weighting stabilized minority predictions.",
        },
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-10",
            "pipeline_stage": "distillation",
            "model": "distilroberta-base",
            "variant": "teacher-xlmr",
            "num_train_epochs": 4.0,
            "batch_size": 32,
            "learning_rate": 2.1e-05,
            "weight_decay": 0.012,
            "max_seq_length": 256,
            "warmup_steps": 150,
            "scheduler": "polynomial",
            "grad_accum": 2,
            "train_samples": 4955,
            "val_accuracy": 0.8894,
            "val_f1_macro": 0.8762,
            "val_precision_macro": 0.8817,
            "val_recall_macro": 0.8741,
            "val_loss": 0.4075,
            "test_accuracy": 0.8831,
            "test_f1_macro": 0.8714,
            "test_precision_macro": 0.8742,
            "test_recall_macro": 0.8692,
            "test_loss": 0.5183,
            "runtime_minutes": 1.28,
            "notes": "Distillation from best XLM-R checkpoint with temperature=2.5.",
        },
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-11",
            "pipeline_stage": "domain-adaptation",
            "model": "xlm-roberta-base",
            "variant": "mlm-news-10k",
            "num_train_epochs": 2.5,
            "batch_size": 16,
            "learning_rate": 1.85e-05,
            "weight_decay": 0.019,
            "max_seq_length": 320,
            "warmup_steps": 120,
            "scheduler": "linear",
            "grad_accum": 2,
            "train_samples": 4955,
            "val_accuracy": 0.9037,
            "val_f1_macro": 0.9112,
            "val_precision_macro": 0.9153,
            "val_recall_macro": 0.9074,
            "val_loss": 0.3895,
            "test_accuracy": 0.8979,
            "test_f1_macro": 0.9061,
            "test_precision_macro": 0.9097,
            "test_recall_macro": 0.9028,
            "test_loss": 0.5057,
            "runtime_minutes": 1.97,
            "notes": "Continued MLM on 10k in-domain reviews prior to supervised fine-tune.",
        },
        {
            "owner": "Candelaria",
            "experiment_id": "CAN-12",
            "pipeline_stage": "ensembling",
            "model": "stacked",
            "variant": "xlmr+tfidf",
            "num_train_epochs": 1.0,
            "batch_size": 0,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "n/a",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.9365,
            "val_f1_macro": 0.9413,
            "val_precision_macro": 0.9448,
            "val_recall_macro": 0.9385,
            "val_loss": 0.3314,
            "test_accuracy": 0.9317,
            "test_f1_macro": 0.9368,
            "test_precision_macro": 0.9393,
            "test_recall_macro": 0.9349,
            "test_loss": 0.4496,
            "runtime_minutes": 0.92,
            "notes": "Stacked ensemble blending TF-IDF and XLM-R logits with logistic meta-learner.",
        },
    ]


def _records_posadas() -> List[Mapping[str, object]]:
    return [
        {
            "owner": "Posadas",
            "experiment_id": "POS-01",
            "pipeline_stage": "data-prep",
            "model": "tfidf-svm",
            "variant": "baseline",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8012,
            "val_f1_macro": 0.7994,
            "val_precision_macro": 0.8038,
            "val_recall_macro": 0.7951,
            "val_loss": 0.5036,
            "test_accuracy": 0.7968,
            "test_f1_macro": 0.7945,
            "test_precision_macro": 0.7981,
            "test_recall_macro": 0.7917,
            "test_loss": 0.5978,
            "runtime_minutes": 0.44,
            "notes": "Baseline linear SVM with TF-IDF and default tokenization.",
        },
        {
            "owner": "Posadas",
            "experiment_id": "POS-02",
            "pipeline_stage": "feature-search",
            "model": "tfidf-svm",
            "variant": "char_2_4",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8174,
            "val_f1_macro": 0.8248,
            "val_precision_macro": 0.8263,
            "val_recall_macro": 0.8229,
            "val_loss": 0.4781,
            "test_accuracy": 0.8127,
            "test_f1_macro": 0.8201,
            "test_precision_macro": 0.8215,
            "test_recall_macro": 0.8184,
            "test_loss": 0.5795,
            "runtime_minutes": 0.46,
            "notes": "Character n-grams (2-4) added; slight gain on macro F1.",
        },
        {
            "owner": "Posadas",
            "experiment_id": "POS-03",
            "pipeline_stage": "feature-search",
            "model": "tfidf-svm",
            "variant": "sublinear_tf",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8291,
            "val_f1_macro": 0.8354,
            "val_precision_macro": 0.8386,
            "val_recall_macro": 0.8319,
            "val_loss": 0.4638,
            "test_accuracy": 0.8257,
            "test_f1_macro": 0.8321,
            "test_precision_macro": 0.8354,
            "test_recall_macro": 0.8288,
            "test_loss": 0.5632,
            "runtime_minutes": 0.47,
            "notes": "Sublinear TF scaling with IDF smoothing.",
        },
        {
            "owner": "Posadas",
            "experiment_id": "POS-04",
            "pipeline_stage": "augmentation",
            "model": "tfidf-svm",
            "variant": "backtranslation",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 5480,
            "val_accuracy": 0.8382,
            "val_f1_macro": 0.8469,
            "val_precision_macro": 0.8475,
            "val_recall_macro": 0.8441,
            "val_loss": 0.4527,
            "test_accuracy": 0.8344,
            "test_f1_macro": 0.8426,
            "test_precision_macro": 0.8448,
            "test_recall_macro": 0.8395,
            "test_loss": 0.5518,
            "runtime_minutes": 0.69,
            "notes": "Englishâ†”Tagalog backtranslation augmentation for minority classes.",
        },
        {
            "owner": "Posadas",
            "experiment_id": "POS-05",
            "pipeline_stage": "transformer",
            "model": "xlm-roberta-base",
            "variant": "frozen-encoder",
            "num_train_epochs": 2.0,
            "batch_size": 32,
            "learning_rate": 2.5e-05,
            "weight_decay": 0.01,
            "max_seq_length": 256,
            "warmup_steps": 100,
            "scheduler": "linear",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8647,
            "val_f1_macro": 0.8725,
            "val_precision_macro": 0.8716,
            "val_recall_macro": 0.8732,
            "val_loss": 0.4379,
            "test_accuracy": 0.8615,
            "test_f1_macro": 0.8683,
            "test_precision_macro": 0.8692,
            "test_recall_macro": 0.8667,
            "test_loss": 0.5336,
            "runtime_minutes": 1.34,
            "notes": "Encoder frozen for first epoch; classifier head trained warm-up.",
        },
        {
            "owner": "Posadas",
            "experiment_id": "POS-06",
            "pipeline_stage": "transformer",
            "model": "xlm-roberta-base",
            "variant": "layer-wise-lr",
            "num_train_epochs": 3.0,
            "batch_size": 16,
            "learning_rate": 1.8e-05,
            "weight_decay": 0.018,
            "max_seq_length": 256,
            "warmup_steps": 200,
            "scheduler": "linear",
            "grad_accum": 2,
            "train_samples": 4955,
            "val_accuracy": 0.9078,
            "val_f1_macro": 0.9129,
            "val_precision_macro": 0.9154,
            "val_recall_macro": 0.9098,
            "val_loss": 0.3825,
            "test_accuracy": 0.9013,
            "test_f1_macro": 0.9071,
            "test_precision_macro": 0.9095,
            "test_recall_macro": 0.9043,
            "test_loss": 0.4942,
            "runtime_minutes": 2.08,
            "notes": "Layer-wise LR decay (0.9) with gradient checkpointing enabled.",
        },
        {
            "owner": "Posadas",
            "experiment_id": "POS-07",
            "pipeline_stage": "transformer",
            "model": "xlm-roberta-large",
            "variant": "full-finetune",
            "num_train_epochs": 2.4,
            "batch_size": 8,
            "learning_rate": 9.8e-06,
            "weight_decay": 0.027,
            "max_seq_length": 320,
            "warmup_steps": 300,
            "scheduler": "cosine",
            "grad_accum": 4,
            "train_samples": 4955,
            "val_accuracy": 0.9186,
            "val_f1_macro": 0.9251,
            "val_precision_macro": 0.9273,
            "val_recall_macro": 0.9219,
            "val_loss": 0.3614,
            "test_accuracy": 0.9128,
            "test_f1_macro": 0.9206,
            "test_precision_macro": 0.9231,
            "test_recall_macro": 0.9184,
            "test_loss": 0.4823,
            "runtime_minutes": 2.87,
            "notes": "Large model fine-tune with sequence length 320 and fp16.",
        },
        {
            "owner": "Posadas",
            "experiment_id": "POS-08",
            "pipeline_stage": "regularization",
            "model": "xlm-roberta-base",
            "variant": "mixout-0.15",
            "num_train_epochs": 3.1,
            "batch_size": 16,
            "learning_rate": 2.2e-05,
            "weight_decay": 0.05,
            "max_seq_length": 256,
            "warmup_steps": 180,
            "scheduler": "cosine",
            "grad_accum": 2,
            "train_samples": 4955,
            "val_accuracy": 0.8997,
            "val_f1_macro": 0.9062,
            "val_precision_macro": 0.9106,
            "val_recall_macro": 0.9019,
            "val_loss": 0.3948,
            "test_accuracy": 0.8951,
            "test_f1_macro": 0.9028,
            "test_precision_macro": 0.9057,
            "test_recall_macro": 0.8996,
            "test_loss": 0.5027,
            "runtime_minutes": 2.21,
            "notes": "Applied mixout regularization=0.15 with R-Drop penalty 0.5.",
        },
        {
            "owner": "Posadas",
            "experiment_id": "POS-09",
            "pipeline_stage": "distillation",
            "model": "miniLM",
            "variant": "teacher-xlmr",
            "num_train_epochs": 4.0,
            "batch_size": 32,
            "learning_rate": 3.1e-05,
            "weight_decay": 0.015,
            "max_seq_length": 256,
            "warmup_steps": 120,
            "scheduler": "linear",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8723,
            "val_f1_macro": 0.8611,
            "val_precision_macro": 0.8678,
            "val_recall_macro": 0.8583,
            "val_loss": 0.4159,
            "test_accuracy": 0.8675,
            "test_f1_macro": 0.8572,
            "test_precision_macro": 0.8616,
            "test_recall_macro": 0.8549,
            "test_loss": 0.5214,
            "runtime_minutes": 1.36,
            "notes": "Distilled MiniLM from POS-07 teacher with KL factor 0.5.",
        },
        {
            "owner": "Posadas",
            "experiment_id": "POS-10",
            "pipeline_stage": "domain-adaptation",
            "model": "xlm-roberta-base",
            "variant": "mlm-social-5k",
            "num_train_epochs": 2.2,
            "batch_size": 16,
            "learning_rate": 1.7e-05,
            "weight_decay": 0.021,
            "max_seq_length": 320,
            "warmup_steps": 140,
            "scheduler": "linear",
            "grad_accum": 2,
            "train_samples": 4955,
            "val_accuracy": 0.9089,
            "val_f1_macro": 0.9157,
            "val_precision_macro": 0.9185,
            "val_recall_macro": 0.9126,
            "val_loss": 0.3796,
            "test_accuracy": 0.9038,
            "test_f1_macro": 0.9104,
            "test_precision_macro": 0.9132,
            "test_recall_macro": 0.9078,
            "test_loss": 0.4907,
            "runtime_minutes": 1.91,
            "notes": "Secondary MLM on 5k social media posts improves robustness to slang.",
        },
        {
            "owner": "Posadas",
            "experiment_id": "POS-11",
            "pipeline_stage": "ensembling",
            "model": "soft-vote",
            "variant": "xlmr+svm",
            "num_train_epochs": 1.0,
            "batch_size": 0,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "n/a",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.9264,
            "val_f1_macro": 0.9317,
            "val_precision_macro": 0.9341,
            "val_recall_macro": 0.9292,
            "val_loss": 0.3446,
            "test_accuracy": 0.9193,
            "test_f1_macro": 0.9245,
            "test_precision_macro": 0.9267,
            "test_recall_macro": 0.9226,
            "test_loss": 0.4712,
            "runtime_minutes": 0.88,
            "notes": "Soft-voting ensemble between POS-06 and POS-03 predictions.",
        },
        {
            "owner": "Posadas",
            "experiment_id": "POS-12",
            "pipeline_stage": "calibration",
            "model": "xlm-roberta-base",
            "variant": "temperature-1.7",
            "num_train_epochs": 0.5,
            "batch_size": 64,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "n/a",
            "grad_accum": 1,
            "train_samples": 800,
            "val_accuracy": 0.9126,
            "val_f1_macro": 0.9194,
            "val_precision_macro": 0.9226,
            "val_recall_macro": 0.9165,
            "val_loss": 0.3728,
            "test_accuracy": 0.9078,
            "test_f1_macro": 0.9141,
            "test_precision_macro": 0.9163,
            "test_recall_macro": 0.9117,
            "test_loss": 0.4865,
            "runtime_minutes": 0.37,
            "notes": "Post-hoc temperature scaling (T=1.7) on validation logits only.",
        },
    ]


def _records_tumulak() -> List[Mapping[str, object]]:
    baseline_records: List[Mapping[str, object]] = [
        {
            "owner": "Tumulak",
            "experiment_id": "TUM-01",
            "pipeline_stage": "data-prep",
            "model": "tfidf-logreg",
            "variant": "baseline",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8049,
            "val_f1_macro": 0.8021,
            "val_precision_macro": 0.8054,
            "val_recall_macro": 0.7992,
            "val_loss": 0.4971,
            "test_accuracy": 0.8013,
            "test_f1_macro": 0.7996,
            "test_precision_macro": 0.8028,
            "test_recall_macro": 0.7965,
            "test_loss": 0.5921,
            "runtime_minutes": 0.46,
            "notes": "Default preprocessing pipeline baseline run.",
        },
        {
            "owner": "Tumulak",
            "experiment_id": "TUM-02",
            "pipeline_stage": "feature-search",
            "model": "tfidf-logreg",
            "variant": "word_1_3",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8386,
            "val_f1_macro": 0.8463,
            "val_precision_macro": 0.8481,
            "val_recall_macro": 0.8442,
            "val_loss": 0.4629,
            "test_accuracy": 0.8342,
            "test_f1_macro": 0.8421,
            "test_precision_macro": 0.8447,
            "test_recall_macro": 0.8396,
            "test_loss": 0.5485,
            "runtime_minutes": 0.5,
            "notes": "Extended word n-grams up to tri-gram with hashing trick.",
        },
        {
            "owner": "Tumulak",
            "experiment_id": "TUM-03",
            "pipeline_stage": "feature-search",
            "model": "tfidf-logreg",
            "variant": "char_word_hybrid",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8441,
            "val_f1_macro": 0.8527,
            "val_precision_macro": 0.8543,
            "val_recall_macro": 0.8506,
            "val_loss": 0.4562,
            "test_accuracy": 0.8408,
            "test_f1_macro": 0.8489,
            "test_precision_macro": 0.8501,
            "test_recall_macro": 0.8473,
            "test_loss": 0.5416,
            "runtime_minutes": 0.52,
            "notes": "Hybrid char+word TF-IDF using 300k features.",
        },
        {
            "owner": "Tumulak",
            "experiment_id": "TUM-04",
            "pipeline_stage": "augmentation",
            "model": "tfidf-logreg",
            "variant": "eda-noise",
            "num_train_epochs": 1.0,
            "batch_size": 128,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "constant",
            "grad_accum": 1,
            "train_samples": 5302,
            "val_accuracy": 0.8217,
            "val_f1_macro": 0.8298,
            "val_precision_macro": 0.8324,
            "val_recall_macro": 0.8281,
            "val_loss": 0.4747,
            "test_accuracy": 0.8173,
            "test_f1_macro": 0.8254,
            "test_precision_macro": 0.8286,
            "test_recall_macro": 0.8227,
            "test_loss": 0.5592,
            "runtime_minutes": 0.64,
            "notes": "Easy Data Augmentation with synonym replacement and random swaps.",
        },
        {
            "owner": "Tumulak",
            "experiment_id": "TUM-05",
            "pipeline_stage": "transformer",
            "model": "xlm-roberta-base",
            "variant": "baseline-fp16",
            "num_train_epochs": 3.0,
            "batch_size": 16,
            "learning_rate": 3.0e-05,
            "weight_decay": 0.01,
            "max_seq_length": 256,
            "warmup_steps": 150,
            "scheduler": "linear",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.8721,
            "val_f1_macro": 0.8814,
            "val_precision_macro": 0.8842,
            "val_recall_macro": 0.8783,
            "val_loss": 0.4306,
            "test_accuracy": 0.8687,
            "test_f1_macro": 0.8782,
            "test_precision_macro": 0.8796,
            "test_recall_macro": 0.8765,
            "test_loss": 0.5244,
            "runtime_minutes": 1.52,
            "notes": "Baseline XLM-R with fp16 and gradient clipping 1.0.",
        },
        {
            "owner": "Tumulak",
            "experiment_id": "TUM-06",
            "pipeline_stage": "transformer",
            "model": "xlm-roberta-base",
            "variant": "rdrop-0.5",
            "num_train_epochs": 3.2,
            "batch_size": 12,
            "learning_rate": 2.4e-05,
            "weight_decay": 0.02,
            "max_seq_length": 256,
            "warmup_steps": 220,
            "scheduler": "cosine",
            "grad_accum": 2,
            "train_samples": 4955,
            "val_accuracy": 0.8945,
            "val_f1_macro": 0.9019,
            "val_precision_macro": 0.9048,
            "val_recall_macro": 0.8991,
            "val_loss": 0.3974,
            "test_accuracy": 0.8893,
            "test_f1_macro": 0.8975,
            "test_precision_macro": 0.8997,
            "test_recall_macro": 0.8958,
            "test_loss": 0.5056,
            "runtime_minutes": 1.94,
            "notes": "R-Drop regularization lambda=0.5; dropout raised to 0.25.",
        },
        {
            "owner": "Tumulak",
            "experiment_id": "TUM-07",
            "pipeline_stage": "transformer",
            "model": "xlm-roberta-base",
            "variant": "adversarial-fgsm",
            "num_train_epochs": 2.6,
            "batch_size": 16,
            "learning_rate": 2.0e-05,
            "weight_decay": 0.017,
            "max_seq_length": 256,
            "warmup_steps": 180,
            "scheduler": "linear",
            "grad_accum": 2,
            "train_samples": 4955,
            "val_accuracy": 0.8881,
            "val_f1_macro": 0.8936,
            "val_precision_macro": 0.8961,
            "val_recall_macro": 0.8912,
            "val_loss": 0.4043,
            "test_accuracy": 0.8827,
            "test_f1_macro": 0.8894,
            "test_precision_macro": 0.8913,
            "test_recall_macro": 0.8875,
            "test_loss": 0.5129,
            "runtime_minutes": 1.78,
            "notes": "FGSM adversarial step eps=5e-4 improved robustness.",
        },
        {
            "owner": "Tumulak",
            "experiment_id": "TUM-08",
            "pipeline_stage": "transformer",
            "model": "xlm-roberta-large",
            "variant": "mixed-precision",
            "num_train_epochs": 2.2,
            "batch_size": 8,
            "learning_rate": 1.2e-05,
            "weight_decay": 0.025,
            "max_seq_length": 320,
            "warmup_steps": 260,
            "scheduler": "cosine",
            "grad_accum": 4,
            "train_samples": 4955,
            "val_accuracy": 0.9168,
            "val_f1_macro": 0.9237,
            "val_precision_macro": 0.9251,
            "val_recall_macro": 0.9218,
            "val_loss": 0.3659,
            "test_accuracy": 0.9109,
            "test_f1_macro": 0.9184,
            "test_precision_macro": 0.9203,
            "test_recall_macro": 0.9162,
            "test_loss": 0.4869,
            "runtime_minutes": 2.65,
            "notes": "Large variant with mixed precision and gradient checkpointing.",
        },
        {
            "owner": "Tumulak",
            "experiment_id": "TUM-09",
            "pipeline_stage": "distillation",
            "model": "distilroberta-base",
            "variant": "teacher-xlmr",
            "num_train_epochs": 3.8,
            "batch_size": 32,
            "learning_rate": 2.6e-05,
            "weight_decay": 0.014,
            "max_seq_length": 256,
            "warmup_steps": 140,
            "scheduler": "polynomial",
            "grad_accum": 2,
            "train_samples": 4955,
            "val_accuracy": 0.8784,
            "val_f1_macro": 0.8679,
            "val_precision_macro": 0.8725,
            "val_recall_macro": 0.8641,
            "val_loss": 0.4226,
            "test_accuracy": 0.8721,
            "test_f1_macro": 0.8624,
            "test_precision_macro": 0.8657,
            "test_recall_macro": 0.8593,
            "test_loss": 0.5188,
            "runtime_minutes": 1.22,
            "notes": "Distillation using KL temperature=2 and hard label mixing 0.3.",
        },
        {
            "owner": "Tumulak",
            "experiment_id": "TUM-10",
            "pipeline_stage": "domain-adaptation",
            "model": "xlm-roberta-base",
            "variant": "mlm-review-8k",
            "num_train_epochs": 2.4,
            "batch_size": 16,
            "learning_rate": 1.6e-05,
            "weight_decay": 0.02,
            "max_seq_length": 320,
            "warmup_steps": 160,
            "scheduler": "linear",
            "grad_accum": 2,
            "train_samples": 4955,
            "val_accuracy": 0.9056,
            "val_f1_macro": 0.9128,
            "val_precision_macro": 0.9151,
            "val_recall_macro": 0.9104,
            "val_loss": 0.3817,
            "test_accuracy": 0.8998,
            "test_f1_macro": 0.9079,
            "test_precision_macro": 0.9102,
            "test_recall_macro": 0.9054,
            "test_loss": 0.4931,
            "runtime_minutes": 1.84,
            "notes": "Continual pretraining on 8k domain reviews prior to supervised fine-tune.",
        },
        {
            "owner": "Tumulak",
            "experiment_id": "TUM-11",
            "pipeline_stage": "ensembling",
            "model": "weighted-blend",
            "variant": "xlmr+distil",
            "num_train_epochs": 1.0,
            "batch_size": 0,
            "learning_rate": 0.0,
            "weight_decay": 0.0,
            "max_seq_length": 0,
            "warmup_steps": 0,
            "scheduler": "n/a",
            "grad_accum": 1,
            "train_samples": 4955,
            "val_accuracy": 0.9318,
            "val_f1_macro": 0.9374,
            "val_precision_macro": 0.9399,
            "val_recall_macro": 0.9351,
            "val_loss": 0.3385,
            "test_accuracy": 0.9262,
            "test_f1_macro": 0.9328,
            "test_precision_macro": 0.9346,
            "test_recall_macro": 0.9309,
            "test_loss": 0.4567,
            "runtime_minutes": 0.79,
            "notes": "Weighted blending (0.7/0.3) between TUM-08 and TUM-09 logits.",
        },
    ]

    analysis_record = {
        "owner": "Tumulak",
        "experiment_id": "TUM-12",
        "pipeline_stage": "analysis",
        "model": "xlm-roberta-base",
        "variant": "error-analysis",
        "num_train_epochs": 0.0,
        "batch_size": 0,
        "learning_rate": 0.0,
        "weight_decay": 0.0,
        "max_seq_length": 0,
        "warmup_steps": 0,
        "scheduler": "n/a",
        "grad_accum": 1,
        "train_samples": 200,
        "val_accuracy": 0.0,
        "val_f1_macro": 0.0,
        "val_precision_macro": 0.0,
        "val_recall_macro": 0.0,
        "val_loss": 0.0,
        "test_accuracy": 0.0,
        "test_f1_macro": 0.0,
        "test_precision_macro": 0.0,
        "test_recall_macro": 0.0,
        "test_loss": 0.0,
        "runtime_minutes": 0.12,
        "notes": "Qualitative error bucketing and confusion analysis (metrics N/A).",
    }

    return baseline_records + [analysis_record]


def _apply_conditional_formatting(path: Path, columns: Iterable[str]) -> None:
    wb = load_workbook(path)
    ws = wb["Experiments"]
    max_row = ws.max_row
    columns = list(columns)

    positive_cols = {
        "val_accuracy",
        "val_f1_macro",
        "val_precision_macro",
        "val_recall_macro",
        "test_accuracy",
        "test_f1_macro",
        "test_precision_macro",
        "test_recall_macro",
    }
    negative_cols = {"val_loss", "test_loss", "runtime_minutes"}

    pos_rule = ColorScaleRule(
        start_type="min",
        start_color="F8696B",
        mid_type="percentile",
        mid_value=50,
        mid_color="FFEB84",
        end_type="max",
        end_color="63BE7B",
    )
    neg_rule = ColorScaleRule(
        start_type="min",
        start_color="63BE7B",
        mid_type="percentile",
        mid_value=50,
        mid_color="FFEB84",
        end_type="max",
        end_color="F8696B",
    )

    for column in positive_cols:
        if column not in columns:
            continue
        idx = columns.index(column) + 1
        letter = get_column_letter(idx)
        ws.conditional_formatting.add(f"{letter}2:{letter}{max_row}", pos_rule)

    for column in negative_cols:
        if column not in columns:
            continue
        idx = list(columns).index(column) + 1
        letter = get_column_letter(idx)
        ws.conditional_formatting.add(f"{letter}2:{letter}{max_row}", neg_rule)

    wb.save(path)


def _save_exports(name: str, rows: List[Mapping[str, object]]) -> None:
    df = pd.DataFrame(rows, columns=COLUMNS)
    out_dir = Path("exports")
    out_dir.mkdir(exist_ok=True)

    csv_path = out_dir / f"{name}_Experiments_IEEE.csv"
    xlsx_path = out_dir / f"{name}_Experiments_IEEE.xlsx"

    df.to_csv(csv_path, index=False, float_format="%.4f")
    with pd.ExcelWriter(xlsx_path, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="Experiments")

    _apply_conditional_formatting(xlsx_path, df.columns)


def main() -> None:
    _save_exports("Candelaria", _records_candelaria())
    _save_exports("Posadas", _records_posadas())
    _save_exports("Tumulak", _records_tumulak())


if __name__ == "__main__":
    main()


